{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss Functionality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, middle_width, num_classes):\n",
    "\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('hidden_layer', nn.Linear(input_size, middle_width)),\n",
    "            ('hidden_activation', nn.ReLU()),\n",
    "        ]))\n",
    "        self.readout = nn.Linear(middle_width, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.readout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Base torch library\n",
    "from torch.utils.data import DataLoader  # Minibathces\n",
    "import torchvision.datasets as datasets  # MNIST dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "def mnist_dataset(batch_size, train=True, values=list(range(10))):\n",
    "    # Initializing MNIST data set.\n",
    "    dataset = datasets.MNIST(root='dataset/', train=train, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "    targets_list = dataset.targets.tolist()\n",
    "    values_index = [i for i in range(len(dataset)) if targets_list[i] in values]\n",
    "\n",
    "    # Creating a subset of ### MNIST targets.\n",
    "    subset = torch.utils.data.Subset(dataset, values_index)\n",
    "    loader = DataLoader(dataset=subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, device, model, loss_function, optimizer_function, values=list(range(10))):\n",
    "    # Training on each data point.\n",
    "\n",
    "    # Set array full of zeros.\n",
    "    kernel_alignments = torch.zeros(len(loader))\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = data.reshape(data.shape[0], -1).to(device=device)\n",
    "        targets = targets[1].to(torch.float32).to(device=device)\n",
    "\n",
    "        # Forwards.\n",
    "        scores = model(data)\n",
    "        # loss = loss_function(scores, classify_targets(targets, values))\n",
    "\n",
    "        labels = one_hot(targets.long() % 10).to(torch.float32)\n",
    "        print(f\" Scores: {scores.size()} and {scores}\")\n",
    "        \n",
    "        print(f\" Labels: {labels.size()} and {labels}\")\n",
    "        print(labels)\n",
    "        output = loss_function(scores, labels)\n",
    "\n",
    "        # Backwards.\n",
    "        optimizer_function.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "        optimizer_function.step()\n",
    "        phi = model.features(data)\n",
    "        print(\"PHI:\")\n",
    "        print(phi.size())\n",
    "        print(\"TARGETS:\" )\n",
    "        print(targets.size())\n",
    "\n",
    "        kernel_alignments[batch_idx] = kernel_calc(targets, phi)\n",
    "\n",
    "    # return torch.mean(kernel_alignments).item(), torch.std(kernel_alignments).item()/len(kernel_alignments)\n",
    "    # return mean and STD or STE of kernel alignment\n",
    "\n",
    "\n",
    "def record_accuracy(device, model, train_loader, test_loader, epoch, ste, mean, values=list(range(10))):\n",
    "    epoch_accuracy = np.array([[\n",
    "        epoch + 1,\n",
    "        check_accuracy(device, model, train_loader, values).cpu(),\n",
    "        check_accuracy(device, model, test_loader, values).cpu(),\n",
    "        mean,\n",
    "        ste\n",
    "    ]])\n",
    "\n",
    "    return epoch_accuracy\n",
    "\n",
    "\n",
    "def check_accuracy(device, model, loader, values=list(range(10))):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = classify_targets(y, values).to(device=device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x)\n",
    "            # 64images x 10,\n",
    "\n",
    "            predictions = scores.argmax(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "def classify_targets(targets, values):\n",
    "    new_targets = targets.clone()\n",
    "\n",
    "    # Changing targets to a classifiable number.\n",
    "    for key, element in enumerate(values):\n",
    "        new_targets[targets == element] = key\n",
    "    return new_targets\n",
    "\n",
    "\n",
    "# Kernel Alignment Fucntions\n",
    "\n",
    "def kernel_calc(y, phi):\n",
    "\n",
    "    # Output Kernel\n",
    "    y = torch.t(torch.unsqueeze(y, -1))\n",
    "    K1 = torch.matmul(torch.t(y), y)\n",
    "    K1c = kernel_centering(K1.float())\n",
    "\n",
    "    # Feature Kernel\n",
    "    K2 = torch.mm(phi, torch.t(phi))\n",
    "    K2c = kernel_centering(K2)\n",
    "\n",
    "    return kernel_alignment(K1c, K2c)\n",
    "\n",
    "\n",
    "def frobenius_product(K1, K2):\n",
    "    return torch.trace(torch.mm(K2, torch.t(K1)))\n",
    "\n",
    "\n",
    "def kernel_alignment(K1, K2):\n",
    "    return frobenius_product(K1, K2) / ((torch.norm(K1, p='fro') * torch.norm(K2, p='fro')))\n",
    "\n",
    "\n",
    "def kernel_centering(K):\n",
    "    # Lemmna 1\n",
    "\n",
    "    m = K.size()[0]\n",
    "    I = torch.eye(m)\n",
    "    l = torch.ones(m, 1)\n",
    "\n",
    "    # I - ll^T / m\n",
    "    mat = I - torch.matmul(l, torch.t(l)) / m\n",
    "\n",
    "    return torch.matmul(torch.matmul(mat, K), mat)\n",
    "\n",
    "\n",
    "def ones(vector):\n",
    "    for i in range(vector.size()[1]):\n",
    "        if vector[0][i] == 9:\n",
    "            vector[0][i] = 1\n",
    "        elif vector[0][i] == 8:\n",
    "            vector[0][i] = -1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = NN(784, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST digits [2, 7]\n"
     ]
    }
   ],
   "source": [
    "# Loading MNIST values\n",
    "train_loader = mnist_dataset(7)\n",
    "validate_loader = mnist_dataset(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss = nn.MSELoss() #!! REPLACE W/ MSELOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD([{'params': model.features.hidden_layer.parameters()},\n",
    "                          {'params': model.readout.parameters(),\n",
    "                           'lr': 0.1}],\n",
    "                         lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty array for storing accuracy metrics\n",
    "import numpy as np\n",
    "accuracy = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scores: torch.Size([7, 10]) and tensor([[-0.0443, -0.0039,  0.0969, -0.0415,  0.0043, -0.0143, -0.0605,  0.1720,\n",
      "          0.0308,  0.1102],\n",
      "        [-0.1027, -0.0835,  0.0890,  0.0021, -0.0115, -0.0006, -0.0220,  0.0706,\n",
      "          0.0942,  0.0448],\n",
      "        [-0.0499,  0.0339,  0.0501,  0.0044,  0.0005, -0.0270, -0.0042,  0.0668,\n",
      "         -0.0091,  0.1061],\n",
      "        [-0.1128, -0.0676,  0.0688, -0.0627, -0.0384, -0.0238, -0.1321,  0.0815,\n",
      "          0.0197,  0.0836],\n",
      "        [-0.0815,  0.0172,  0.0584, -0.0229, -0.0379,  0.0453, -0.0857,  0.0100,\n",
      "          0.0651,  0.0497],\n",
      "        [-0.0423, -0.0007,  0.1351, -0.0120, -0.0552, -0.0140, -0.0430,  0.1131,\n",
      "          0.0523,  0.1119],\n",
      "        [-0.0478, -0.0577,  0.1366,  0.0322, -0.0430, -0.0667, -0.1478,  0.0271,\n",
      "          0.0730,  0.0165]], grad_fn=<AddmmBackward0>)\n",
      " Labels: torch.Size([3]) and tensor([0., 0., 1.])\n",
      "tensor([0., 0., 1.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_loader, device, model, loss, optimizer)\n",
      "\u001b[1;32m/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, device, model, loss_function, optimizer_function, values)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Labels: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m output \u001b[39m=\u001b[39m loss_function(scores, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Backwards.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cameronkaminski/glomerulus-lab/slow_feature_learning/notebooks/mseloss_test.ipynb#X46sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m optimizer_function\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/slow-feature-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/slow-feature-learning/lib/python3.9/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/anaconda3/envs/slow-feature-learning/lib/python3.9/site-packages/torch/nn/functional.py:3291\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3289\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3291\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3292\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/anaconda3/envs/slow-feature-learning/lib/python3.9/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train(train_loader, device, model, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy csv\n",
    "import pandas as pd\n",
    "complete_dataframe = pd.DataFrame(accuracy).to_csv('accuracy_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing MNSIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(batch_size, train=True, values=list(range(10))):\n",
    "    # Initializing MNIST data set.\n",
    "    dataset = datasets.MNIST(root='dataset/', train=train, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "    targets_list = dataset.targets.tolist()\n",
    "    values_index = [i for i in range(len(dataset)) if targets_list[i] in values]\n",
    "\n",
    "    # Creating a subset of ### MNIST targets.\n",
    "    subset = torch.utils.data.Subset(dataset, values_index)\n",
    "    loader = DataLoader(dataset=subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root='dataset/', train=train, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(range(0,10))\n",
    "targets_list = dataset.targets.tolist()\n",
    "values_index = [i for i in range(len(dataset)) if targets_list[i] in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subset of ### MNIST targets.\n",
    "batch_size = 200\n",
    "subset = torch.utils.data.Subset(dataset, values_index)\n",
    "loader = DataLoader(dataset=subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets \n",
    "values = [1, 1]\n",
    "batch_size = 12002\n",
    "dataset = datasets.MNIST(root='dataset/', transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "targets_list = dataset.targets.tolist()\n",
    "values_index = [i for i in range(len(dataset)) if targets_list[i] in values]\n",
    "\n",
    "# Creating a subset of ### MNIST targets.\n",
    "subset = torch.utils.data.Subset(dataset, values_index)\n",
    "loader = DataLoader(dataset=subset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data, targets in enumerate(loader):\n",
    "    if i == 0:\n",
    "        t = targets\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 7., 8., 8., 7., 0., 4., 3., 0., 0., 7., 8., 7., 4., 9., 3., 6., 1.,\n",
       "        6., 4., 1., 6., 1., 1., 8., 1., 7., 6., 4., 4., 9., 9., 7., 8., 2., 7.,\n",
       "        1., 9., 6., 2., 9., 3., 1., 7., 6., 2., 3., 3., 0., 6., 9., 5., 9., 4.,\n",
       "        4., 1., 2., 7., 0., 9., 6., 9., 2., 7., 1., 3., 8., 9., 0., 3., 3., 0.,\n",
       "        4., 7., 8., 1., 1., 9., 8., 7., 2., 5., 8., 8., 9., 1., 7., 2., 1., 3.,\n",
       "        8., 0., 0., 8., 6., 6., 7., 2., 1., 4., 2., 0., 2., 7., 8., 0., 1., 4.,\n",
       "        1., 4., 9., 1., 5., 5., 5., 7., 6., 2., 5., 2., 7., 7., 4., 5., 0., 0.,\n",
       "        2., 4., 4., 5., 5., 9., 7., 7., 2., 3., 8., 1., 6., 5., 7., 1., 0., 9.,\n",
       "        1., 1., 3., 4., 6., 0., 0., 5., 0., 2., 9., 4., 9., 8., 7., 0., 2., 7.,\n",
       "        6., 9., 0., 8., 2., 5., 6., 3., 6., 7., 0., 8., 8., 8., 5., 0., 9., 3.,\n",
       "        8., 7., 8., 7., 3., 5., 9., 5., 1., 0., 1., 2., 9., 5., 9., 9., 5., 0.,\n",
       "        9., 8.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 10])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(t.long() % 10).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8044, -0.4141, -0.7076, -1.2489, -0.2123],\n",
       "        [-0.7767,  0.5124,  0.4282, -0.4937,  0.2222],\n",
       "        [ 0.7859,  0.6405,  0.4175,  1.9157,  0.8038]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.randn(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9557, -0.2617, -0.8787, -0.7586,  0.0471],\n",
       "        [-0.8757, -0.7160, -0.1519,  1.9531, -0.3153],\n",
       "        [-0.7677,  1.0559,  0.0542, -0.6688,  1.0281]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,5, requires_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('slow-feature-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf3386a2c431b61bf14b5d0776eee9364949d64460c79cdeca756330e42823a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
