{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network class.\n",
    "Contains the nueral network object w/ all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Class\n",
    "\n",
    "import torch.nn as nn # Nueral network module.\n",
    "from collections import OrderedDict # Dictinary module.\n",
    "\n",
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, middle_width, num_classes):\n",
    "\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('hidden_layer', nn.Linear(input_size, middle_width)),\n",
    "            ('hidden_activation', nn.ReLU()),\n",
    "        ]))\n",
    "        self.readout = nn.Linear(middle_width, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.readout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Base torch library\n",
    "from torch.utils.data import DataLoader  # Minibathces\n",
    "import torchvision.datasets as datasets  # MNIST dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "def mnist_dataset(batch_size, train=True, values=list(range(10))):\n",
    "    # Initializing MNIST data set.\n",
    "    dataset = datasets.MNIST(root='dataset/', train=train, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "    targets_list = dataset.targets.tolist()\n",
    "    values_index = [i for i in range(len(dataset)) if targets_list[i] in values]\n",
    "\n",
    "    # Creating a subset of ### MNIST targets.\n",
    "    subset = torch.utils.data.Subset(dataset, values_index)\n",
    "    loader = DataLoader(dataset=subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train(loader, device, model, loss_function, optimizer_function, values=list(range(10))):\n",
    "    # Training on each data point.\n",
    "\n",
    "    # Set array full of zeros.\n",
    "    kernel_alignments = torch.zeros(len(loader))\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = data.reshape(data.shape[0], -1).to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forwards.\n",
    "        scores = model(data)\n",
    "        loss = loss_function(scores, classify_targets(targets, values))\n",
    "\n",
    "        # Backwards.\n",
    "        optimizer_function.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_function.step()\n",
    "        phi = model.features(data)\n",
    "\n",
    "        kernel_alignments[batch_idx] = kernel_calc(targets, phi)\n",
    "\n",
    "    return torch.mean(kernel_alignments).item(), torch.std(kernel_alignments).item()/len(kernel_alignments)\n",
    "    # return mean and STD or STE of kernel alignment\n",
    "\n",
    "\n",
    "def record_accuracy(device, model, train_loader, test_loader, epoch, ste, mean, values=list(range(10))):\n",
    "    epoch_accuracy = np.array([[\n",
    "        epoch + 1,\n",
    "        check_accuracy(device, model, train_loader, values).cpu(),\n",
    "        check_accuracy(device, model, test_loader, values).cpu(),\n",
    "        mean,\n",
    "        ste\n",
    "    ]])\n",
    "\n",
    "    return epoch_accuracy\n",
    "\n",
    "\n",
    "def check_accuracy(device, model, loader, values=list(range(10))):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = classify_targets(y, values).to(device=device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x)\n",
    "            # 64images x 10,\n",
    "\n",
    "            predictions = scores.argmax(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "def classify_targets(targets, values):\n",
    "    new_targets = targets.clone()\n",
    "\n",
    "    # Changing targets to a classifiable number.\n",
    "    for key, element in enumerate(values):\n",
    "        new_targets[targets == element] = key\n",
    "    return new_targets\n",
    "\n",
    "\n",
    "# Kernel Alignment Fucntions\n",
    "\n",
    "def kernel_calc(y, phi):\n",
    "\n",
    "    # Output Kernel\n",
    "    y = torch.t(torch.unsqueeze(y, -1))\n",
    "    K1 = torch.matmul(torch.t(y), y)\n",
    "    K1c = kernel_centering(K1.float())\n",
    "\n",
    "    # Feature Kernel\n",
    "    K2 = torch.mm(phi, torch.t(phi))\n",
    "    K2c = kernel_centering(K2)\n",
    "\n",
    "    return kernel_alignment(K1c, K2c)\n",
    "\n",
    "\n",
    "def frobenius_product(K1, K2):\n",
    "    return torch.trace(torch.mm(K2, torch.t(K1)))\n",
    "\n",
    "\n",
    "def kernel_alignment(K1, K2):\n",
    "    return frobenius_product(K1, K2) / ((torch.norm(K1, p='fro') * torch.norm(K2, p='fro')))\n",
    "\n",
    "\n",
    "def kernel_centering(K):\n",
    "    # Lemmna 1\n",
    "\n",
    "    m = K.size()[0]\n",
    "    I = torch.eye(m)\n",
    "    l = torch.ones(m, 1)\n",
    "\n",
    "    # I - ll^T / m\n",
    "    mat = I - torch.matmul(l, torch.t(l)) / m\n",
    "\n",
    "    return torch.matmul(torch.matmul(mat, K), mat)\n",
    "\n",
    "\n",
    "def ones(vector):\n",
    "    for i in range(vector.size()[1]):\n",
    "        if vector[0][i] == 9:\n",
    "            vector[0][i] = 1\n",
    "        elif vector[0][i] == 8:\n",
    "            vector[0][i] = -1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Hyper Parameters: {'Input Size': 784, 'Middle Layer Width': 2, 'Num Classes': 2, 'Regular Learning Rate': 0.01, 'Slow Learning Rate': 0.001, 'Batch Size': 10, 'Epochs': 20}\n",
      "MNIST digits [2, 7]\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21794/2026568489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m sl_optimizer = optim.SGD([{'params': slow_model.features.hidden_layer.parameters()},\n\u001b[0m\u001b[1;32m     36\u001b[0m                             {'params': slow_model.readout.parameters(),\n\u001b[1;32m     37\u001b[0m                             'lr': hp[\"Regular Learning Rate\"]}],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking & Setting Device Allocation\n",
    "device = set_device()\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "# Hyper Parameters\n",
    "hp = {\n",
    "    \"Input Size\": 784,\n",
    "    \"Middle Layer Width\": 2,\n",
    "    \"Num Classes\": 2,\n",
    "    \"Regular Learning Rate\": 0.01,\n",
    "    \"Slow Learning Rate\": 0.001,\n",
    "    \"Batch Size\": 10,\n",
    "    \"Epochs\": 20\n",
    "}\n",
    "print(f\"Hyper Parameters: {hp}\")\n",
    "\n",
    "# Initializing Model\n",
    "slow_model = NN(input_size=hp[\"Input Size\"],\n",
    "                middle_width=hp[\"Middle Layer Width\"],\n",
    "                num_classes=hp[\"Num Classes\"]).to(device=device)\n",
    "\n",
    "reg_model = NN(input_size=hp[\"Input Size\"],\n",
    "                middle_width=hp[\"Middle Layer Width\"],\n",
    "                num_classes=hp[\"Num Classes\"]).to(device=device)\n",
    "\n",
    "# Loading MNIST Dataset\n",
    "mnist_values = [2, 7]\n",
    "print(f\"MNIST digits {mnist_values}\")\n",
    "train_loader = mnist_dataset(hp[\"Batch Size\"], values=mnist_values)\n",
    "validate_loader = mnist_dataset(hp[\"Batch Size\"], train=False, values=mnist_values)\n",
    "\n",
    "# Loss function\n",
    "loss_function = nn.MSELoss()\n",
    "# Optimizers\n",
    "sl_optimizer = optim.SGD([{'params': slow_model.features.hidden_layer.parameters()},\n",
    "                            {'params': slow_model.readout.parameters(),\n",
    "                            'lr': hp[\"Regular Learning Rate\"]}],\n",
    "                            lr=hp[\"Slow Learning Rate\"])\n",
    "r_optimizer = optim.SGD(reg_model.parameters(), lr=hp[\"Regular Learning Rate\"])\n",
    "\n",
    "# Creating 'empty' arrays for future storing of accuracy metrics\n",
    "slow_accuracy = np.zeros((hp[\"Epochs\"], 5))\n",
    "regular_accuracy = np.zeros((hp[\"Epochs\"], 5))\n",
    "\n",
    "print(\"Training models...\")\n",
    "for epoch in range(hp[\"Epochs\"]):\n",
    "\n",
    "    # Slow Model\n",
    "    sl_mean, sl_ste = train(train_loader, device, slow_model, loss_function, sl_optimizer, values=mnist_values)\n",
    "    slow_accuracy[epoch][0] = epoch + 1\n",
    "    slow_accuracy[epoch][1] = check_accuracy(device, slow_model, train_loader, mnist_values).cpu()\n",
    "    slow_accuracy[epoch][2] = check_accuracy(device, slow_model, validate_loader, mnist_values).cpu()\n",
    "    slow_accuracy[epoch][3] = sl_mean\n",
    "    slow_accuracy[epoch][4] = sl_ste\n",
    "    # CALCULATE THE K.A. AND RECORD IT TO A CSV (FOR SLOW MODEL)\n",
    "    print(\"Slow: \")\n",
    "    print(slow_accuracy[epoch])\n",
    "\n",
    "    # Regular Model\n",
    "    reg_mean, reg_ste = train(train_loader, device, reg_model, loss_function, r_optimizer, values=mnist_values)\n",
    "    regular_accuracy[epoch][0] = epoch + 1\n",
    "    regular_accuracy[epoch][1] = check_accuracy(device, reg_model, train_loader, mnist_values).cpu()\n",
    "    regular_accuracy[epoch][2] = check_accuracy(device, reg_model, validate_loader, mnist_values).cpu()\n",
    "    regular_accuracy[epoch][3] = reg_mean\n",
    "    regular_accuracy[epoch][4] = sl_ste\n",
    "\n",
    "\n",
    "    # CALCULATE THE K.A. AND RECORD IT TO A CSV (FOR REG MODEL)\n",
    "    print(\"Reg: \")\n",
    "    print(regular_accuracy[epoch])\n",
    "    print(f\"-Finished epoch {epoch + 1}/{hp['Epochs']}\")\n",
    "\n",
    "    # compute al. on both t and v.\n",
    "\n",
    "# Accuracy csv\n",
    "complete_array = np.concatenate((slow_accuracy, regular_accuracy), axis=1)\n",
    "complete_dataframe = pd.DataFrame(complete_array).to_csv('../accuracy_metrics')\n",
    "print(f\"-Saved accuracy metrics as 'accuracy_metrics'\")\n",
    "\n",
    "# Saving the entire model\n",
    "torch.save(slow_model.state_dict(), '../slow_model.pt')\n",
    "print(f\"-Saved Regular Model Parameters as 'slow_model.pt'\")\n",
    "torch.save(reg_model.state_dict(), '../reg_model.pt')\n",
    "print(f\"-Saved Regular Model Parameters as 'reg_model.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('slow-feature-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6793bc25953287ce4e1462fd6cd28f236545844af7abf6eea59179aacc7dc944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
